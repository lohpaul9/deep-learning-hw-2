{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WB7znsvhyuRH"
      },
      "outputs": [],
      "source": [
        "# torch and torchvision imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI6XEXSGyYkh"
      },
      "source": [
        "### (a) Plot the training and validation losses and errors as a function of the number of epochs\n",
        "\n",
        "\n",
        " The model currently does not achieve less than 12% validation error, you have to tweak the parameters to get it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "AaNQQai_wON0",
        "outputId": "ab731c3e-cd76-4b98-9322-c30c1ffb6dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 34546657.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reading in the dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# print the shape of the dataset\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# print one of the images\n",
        "img = images[0]\n",
        "img = img / 2 + 0.5     # unnormalize\n",
        "\n",
        "npimg = img.numpy()\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Defining the model\n",
        "class View(nn.Module):\n",
        "    def __init__(self,o):\n",
        "        super().__init__()\n",
        "        self.o = o\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x.view(-1, self.o)\n",
        "\n",
        "class allcnn_t(nn.Module):\n",
        "    def __init__(self, c1=96, c2= 192):\n",
        "        super().__init__()\n",
        "        d = 0.5\n",
        "\n",
        "\n",
        "        def convbn(channels_in,channels_out,kernel_size,stride=1,padding_size=0):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(channels_in,channels_out,kernel_size,stride=stride,padding=padding_size),\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm2d(channels_out))\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            convbn(3,c1,3,1,1),\n",
        "            convbn(c1,c1,3,1,1),\n",
        "            convbn(c1,c1,3,2,1),\n",
        "            nn.Dropout(d),\n",
        "            convbn(c1,c2,3,1,1),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,c2,3,2,1),\n",
        "            nn.Dropout(d),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,10,1,1),\n",
        "            nn.AvgPool2d(8),\n",
        "            View(10))\n",
        "\n",
        "        print('Num parameters: ', sum([p.numel() for p in self.model.parameters()]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "34JavUIgRpcP"
      },
      "outputs": [],
      "source": [
        "# The training loop\n",
        "\n",
        "def train(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot):\n",
        "    model = net.to(device)\n",
        "    total_step = len(train_loader)\n",
        "    overall_step = 0\n",
        "    train_loss_values = []\n",
        "    train_error = []\n",
        "    val_loss_values = []\n",
        "    val_error = []\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        if epoch == 40:\n",
        "          for op_params in optimizer.param_groups:\n",
        "            op_params['lr'] = 0.005\n",
        "        if epoch == 80:\n",
        "          for op_params in optimizer.param_groups:\n",
        "            op_params['lr'] = 0.001\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Move tensors to configured device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #Forward Pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 1000 == 0:\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            if plot:\n",
        "              info = { ('loss_' + model_name): loss.item() }\n",
        "\n",
        "              # for tag, value in info.items():\n",
        "              #   logger.scalar_summary(tag, value, overall_step+1)\n",
        "        train_loss_values.append(running_loss)\n",
        "        train_error.append(100-100*correct/total)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, (images, labels) in enumerate(test_loader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
        "        val_error.append(100-100*correct/total)\n",
        "        val_loss_values.append(running_loss)\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model.state_dict(), f'{model_name}_{epoch}.ckpt')\n",
        "    return val_error,val_loss_values,train_error,train_loss_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y7YvtAlERz61"
      },
      "outputs": [],
      "source": [
        "# model = allcnn_t().to(device)\n",
        "# epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001, nesterov=True)\n",
        "# val_error,val_loss_values,train_error,train_loss_values= train(model, optimizer, criterion, trainloader, testloader, epochs, 'cnn_curve', True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = allcnn_t().to(device)\n",
        "model.load_state_dict(torch.load('cnn_curve_99.ckpt'))"
      ],
      "metadata": {
        "id": "oL9Ijxe2iCSI",
        "outputId": "2dd84dae-a786-46ee-c437-b28b8e802259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num parameters:  1667166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-224920aab95d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('cnn_curve_99.ckpt'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-224920aab95d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallcnn_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn_curve_99.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1098\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     \u001b[0;31m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1466\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mbackend_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_privateuse1_backend_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mdevice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_available'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         raise RuntimeError(f'Attempting to deserialize object on a {backend_name.upper()} '\n\u001b[0m\u001b[1;32m    365\u001b[0m                            \u001b[0;34mf'device but torch.{backend_name}.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the accuracy of the saved model\n",
        "def get_accuracy(model, test_loader):\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for i, (images, labels) in enumerate(test_loader):\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "              outputs = model(images)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "get_accuracy(model, testloader)\n"
      ],
      "metadata": {
        "id": "LcoqVKs-lg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps_pixels = 2.0\n",
        "eps_normalized = (eps_pixels / 255.0) / 0.5\n",
        "\n",
        "def sanity_check_within_epsilon(unperturbed_img, perturbed_img, epsilon):\n",
        "    # change the images back to pixel space\n",
        "    unperturbed_img = unperturbed_img.cpu().detach().numpy()\n",
        "    perturbed_img = perturbed_img.cpu().detach().numpy()\n",
        "    # denormalize\n",
        "    unperturbed_img = (unperturbed_img * 0.5 + 0.5) * 255\n",
        "    perturbed_img = (perturbed_img * 0.5 + 0.5) * 255\n",
        "\n",
        "    # get the difference\n",
        "    difference = unperturbed_img - perturbed_img\n",
        "    difference = np.absolute(difference)\n",
        "\n",
        "    # check if the difference is within epsilon\n",
        "    return np.all(difference <= (epsilon + 0.0001))\n",
        "\n",
        "\n",
        "def adv_gradient_attack(model):\n",
        "    model.eval()\n",
        "    attack_set = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform_test\n",
        "    )\n",
        "    subset_indices = torch.randperm(len(attack_set))[:100]\n",
        "    attack_set = torch.utils.data.Subset(attack_set, subset_indices)\n",
        "    attack_loader = torch.utils.data.DataLoader(attack_set, batch_size=100, shuffle=False)\n",
        "\n",
        "    base_images, labels = next(iter(attack_loader))\n",
        "    base_images = base_images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    base_images.requires_grad = True\n",
        "\n",
        "    perturbed_images_list = [base_images]\n",
        "    imgs = []\n",
        "    accuracies = []\n",
        "\n",
        "    for perturbation_round in range(6):\n",
        "        unperturbed = perturbed_images_list[-1]\n",
        "        model.zero_grad()  # Clear gradients\n",
        "        y = model(unperturbed)\n",
        "        loss = criterion(y, labels)\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(y.data, 1)\n",
        "        total = labels.size(0)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # Store the perturbed image for plotting\n",
        "        img = unperturbed[0].cpu().detach().numpy() / 2 + 0.5  # Unnormalize\n",
        "        imgs.append(img)\n",
        "\n",
        "        loss.backward()\n",
        "        x_grad = unperturbed.grad.data.clone()\n",
        "        x_grad_sign = x_grad.sign()\n",
        "        perturbation = x_grad_sign * eps_normalized\n",
        "\n",
        "\n",
        "        # Apply perturbation and clamp values between 0 and 1\n",
        "        perturbed = torch.clamp(unperturbed + perturbation, -1, 1).detach()\n",
        "\n",
        "        print(\"All perturbations within sanity check\", sanity_check_within_epsilon(unperturbed, perturbed, eps_pixels))\n",
        "        # assert sanity_check_within_epsilon(unperturbed, perturbed, eps_normalized)\n",
        "\n",
        "        perturbed.requires_grad = True\n",
        "        perturbed_images_list.append(perturbed)\n",
        "\n",
        "        # zero the model gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "    return imgs, accuracies\n",
        "\n",
        "\n",
        "# Run the adversarial attack\n",
        "imgs, accuracies = adv_gradient_attack(model)\n",
        "\n",
        "# Plot only 5 images (1 row x 5 columns)\n",
        "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, (img, ax) in enumerate(zip(imgs, axs)):\n",
        "    ax.imshow(np.transpose(img, (1, 2, 0)))  # Transpose for imshow\n",
        "    ax.set_title(f\"Acc: {accuracies[i]:.2f}%\", fontsize=10)\n",
        "    ax.axis('off')  # Turn off axis\n",
        "\n",
        "# Adjust layout\n",
        "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vC5LSUbDYiNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "PS2_Q5_solution.ipynb",
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}